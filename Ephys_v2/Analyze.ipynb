{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcHMu4GKYe8MRwdCGpS9Tf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtabuena/Workshop/blob/main/Ephys_v2/Analyze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'Get Standard Modules'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from scipy import stats\n",
        "import os\n",
        "from scipy.signal import butter,filtfilt\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from IPython.display import clear_output\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import warnings\n",
        "import shutil\n",
        "from google.colab import files\n",
        "!pip install openpyxl --quiet\n",
        "!pip install XlsxWriter --quiet\n",
        "\n",
        "\n",
        "'''Get Repositories'''\n",
        "try: shutil.rmtree('/content/EphysLib')\n",
        "except: None\n",
        "\n",
        "\"run dtabuena's ephys notebooks\"\n",
        "!git clone https://github.com/dtabuena/EphysLib\n",
        "to_import = [\n",
        "            'ABF_Quality_Control.ipynb',\n",
        "            'Basic_Ephys.ipynb',\n",
        "            'Simple_ABF_tools.ipynb',\n",
        "            'fun_math.ipynb',\n",
        "            'importing_abfs_from_dropbox.ipynb',\n",
        "            'QC_recoding_dataframe.ipynb',\n",
        "            'Analyzers/input_resistance_analyzer.ipynb',\n",
        "            'Analyzers/gain_analyzer.ipynb',\n",
        "            'Analyzers/latencey_analyzer.ipynb',\n",
        "            'Analyzers/IV_analyzer.ipynb',\n",
        "            'Analyzers/Vm_analyzer.ipynb',\n",
        "            'Analyzers/membrane_analyzer.ipynb',\n",
        "            'Analyzers/rheobase_analyzer.ipynb',\n",
        "\n",
        "            'Ephys_wrapper.ipynb',\n",
        "            ]\n",
        "for i in to_import:\n",
        "    f = '/content/EphysLib/' + i\n",
        "    %run $f --quiet\n",
        "\n",
        "\n",
        "##### Setup Protocol List\n",
        "VC_prot = ['VC - MemTest-10ms-160ms',\n",
        "           'VC - Multi IV - 150ms',]\n",
        "IC_prot = ['IC - Gain - D20pA',\n",
        "           'IC - Gain - D50pA',\n",
        "           'IC - Rheobase',\n",
        "           'IC - R input',\n",
        "           'IC - Latentcy 800pA-1s'\n",
        "           'VC - 3min GapFree',\n",
        "           'I0 - 3min GapFree']"
      ],
      "metadata": {
        "id": "-00k7cUjcgJI",
        "outputId": "6d1a3a98-8e09-4474-e61e-15fb96139aa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'EphysLib'...\n",
            "remote: Enumerating objects: 1141, done.\u001b[K\n",
            "remote: Counting objects: 100% (518/518), done.\u001b[K\n",
            "remote: Compressing objects: 100% (247/247), done.\u001b[K\n",
            "remote: Total 1141 (delta 377), reused 359 (delta 271), pack-reused 623\u001b[K\n",
            "Receiving objects: 100% (1141/1141), 17.41 MiB | 15.96 MiB/s, done.\n",
            "Resolving deltas: 100% (754/754), done.\n",
            "Collecting pyabf\n",
            "  Downloading pyabf-2.3.8-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pyabf) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from pyabf) (1.23.5)\n",
            "Requirement already satisfied: pytest>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from pyabf) (7.4.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pyabf) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pyabf) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pyabf) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pyabf) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pyabf) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pyabf) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pyabf) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pyabf) (2.8.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.0.7->pyabf) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.0.7->pyabf) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.0.7->pyabf) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.0.7->pyabf) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pyabf) (1.16.0)\n",
            "Installing collected packages: pyabf\n",
            "Successfully installed pyabf-2.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "def analyze_dataset( dataset,valid_protocols, log_level,spike_args,manual_exclusions=None):\n",
        "    ''' Analyze are recordings in a dataset '''\n",
        "    results_dict={}\n",
        "\n",
        "    ''' Get Dataset '''\n",
        "    file_loc = get_drobox_folder(dataset['data_source'],  dataset['data_name'])\n",
        "    abf_recordings_df, protocol_set = catalogue_recs(file_loc,dataset['file_naming_scheme'])\n",
        "    results_dict['abf_recordings_df'] = abf_recordings_df\n",
        "    results_dict['protocol_set'] = protocol_set\n",
        "    abf_recordings_df, _ = purge_wrong_clamp(abf_recordings_df,VC_prot,IC_prot)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rpKbw17WP9GC",
        "outputId": "222c305d-f692-4030-e79e-3824dc78c59c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Watch out!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {'data_name': 'Wild_Type_Hipp',\n",
        "           'data_source': \"https://www.dropbox.com/scl/fo/xpsvbsuu6nzsl4rtm7fvv/h?rlkey=wj4gdwxalt4wse40um2z2fvjx&dl=1\",\n",
        "           'file_naming_scheme': ['Rec_date','GenoType','Sex','Age','SliceDir','Slice_Num','Cell_num','Cell_Type'],\n",
        "           }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VhsLfA3STFSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import urllib\n",
        "urllib.request.urlretrieve( 'https://github.com/dtabuena/Workshop/raw/1b79dca6d197f411442a908ea00ac5a7267e8247/Ephys_v2/analysis_args.json', \"./analysis_args.json\")\n",
        "with open('./analysis_args.json', 'r') as my_json:\n",
        "    analysis_args = json.load(my_json)"
      ],
      "metadata": {
        "id": "-KGz863vMPKB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_rmp(abf,analysis_args):\n",
        "    pass\n",
        "def analyze_rheobase(abf,analysis_args):\n",
        "    pass\n",
        "def analyze_gain(abf,analysis_args):\n",
        "    pass\n",
        "def analyze_membrane_props(abf,analysis_args):\n",
        "    pass\n",
        "def analyze_latencey(abf,analysis_args):\n",
        "    pass\n",
        "def analyze_R_input(abf,analysis_args):\n",
        "    pass\n",
        "def analyze_IV_curves(abf,analysis_args):\n",
        "    pass"
      ],
      "metadata": {
        "id": "3obrGE5RVOKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "vmVr6tOQDmb_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "5Sno2cTQE780"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_drobox_folder(link, new_filename):\n",
        "    'Download a folder from dropbox and unzip'\n",
        "    zipped_file_path = \"/content/\"+new_filename + \".zip\"\n",
        "    unzipped_file_path = \"/content/\"+new_filename\n",
        "    if not( os.path.exists(zipped_file_path)):\n",
        "        !wget -O $zipped_file_path $link    # download with new name\n",
        "    !echo A | unzip $zipped_file_path -d $unzipped_file_path\n",
        "    return new_filename"
      ],
      "metadata": {
        "id": "EcAfJKqAU6mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCL2F0lBPHRt"
      },
      "outputs": [],
      "source": [
        "def ephys_wrapper(dataset,VC_prot,IC_prot,strat_cols=['Cell_Type'],verbose=False, spike_args={'spike_thresh':20, 'high_dv_thresh': 50,'low_dv_thresh': -30,'window_ms': 2},manual_exclusions=[]):\n",
        "    '''wrapper for single dataset pipeline'''\n",
        "    results = {}\n",
        "    try:\n",
        "        '''Unpack'''\n",
        "        data_name = dataset['data_name']\n",
        "        data_source = dataset['data_source']\n",
        "        file_naming_scheme = dataset['file_naming_scheme']\n",
        "\n",
        "\n",
        "\n",
        "        ''' Gather and Catalog Source Data'''\n",
        "        file_loc = get_drobox_folder(data_source, data_name)\n",
        "        # clear_output(wait=False)\n",
        "        abf_recordings_df, protocol_set = catalogue_recs(file_loc,file_naming_scheme)\n",
        "        results['abf_recordings_df'] = abf_recordings_df\n",
        "        results['protocol_set'] = protocol_set\n",
        "\n",
        "\n",
        "        abf_recordings_df, _ = purge_wrong_clamp(abf_recordings_df,VC_prot,IC_prot)\n",
        "\n",
        "        results['abf_recordings_df'] = abf_recordings_df\n",
        "\n",
        "        csv_name = cell_prot_lut(abf_recordings_df,protocol_set,csv_name=data_name+'_Recording_LookUp')\n",
        "        results['prot_lut'] = csv_name\n",
        "\n",
        "        '''Set Internal Analysis Params'''\n",
        "        print(spike_args)\n",
        "\n",
        "        func_dict = {}\n",
        "        arg_dict = {}\n",
        "\n",
        "        func_dict['VC - 3min GapFree']= rmp_analyzer\n",
        "        arg_dict['VC - 3min GapFree'] = [True] # [to_plot?]\n",
        "\n",
        "        func_dict['I0 - 3min GapFree']= rmp_analyzer\n",
        "        arg_dict['I0 - 3min GapFree'] = [True] # [to_plot?]\n",
        "\n",
        "        func_dict['IC - Rheobase']= rheobase_analyzer\n",
        "        arg_dict['IC - Rheobase'] = [spike_args, True, False, False]  # [spike_args, to_plot, verbose, force_singlespike]\n",
        "\n",
        "        func_dict['IC - Gain - D10pA']= gain_analyzer\n",
        "        arg_dict['IC - Gain - D10pA']= [spike_args, 1, 4, .7,[-60,-80]]  # [spike_args, to_plot [0:2], max_fit_steps, rel_slope_cut, Vh_hilo]\n",
        "\n",
        "        func_dict['IC - Gain - D20pA']= gain_analyzer\n",
        "        arg_dict['IC - Gain - D20pA']= [spike_args, 1, 4, .7,[-60,-80]]  # [spike_args, to_plot [0:2], max_fit_steps, rel_slope_cut, Vh_hilo]\n",
        "\n",
        "        func_dict['IC - Gain - D25pA']= gain_analyzer\n",
        "        arg_dict['IC - Gain - D25pA']= [spike_args, 1, 4, .7,[-60,-80]]  # [spike_args, to_plot [0:2], max_fit_steps, rel_slope_cut, Vh_hilo]\n",
        "\n",
        "        func_dict['IC - Gain - D50pA']= gain_analyzer\n",
        "        arg_dict['IC - Gain - D50pA']= arg_dict['IC - Gain - D20pA']\n",
        "\n",
        "        func_dict['VC - MemTest-10ms-160ms']= membrane_analyzer\n",
        "        arg_dict['VC - MemTest-10ms-160ms']= [True, False, ['Ra', 'Rm', 'Cm', 'tau',\t'Cmq',\t'Cmf',\t'Cmqf', 'Cm_pc']]  # [to_plot, verbose]\n",
        "\n",
        "        func_dict['IC - Latentcy 800pA-1s']= latencey_analyzer\n",
        "        arg_dict['IC - Latentcy 800pA-1s']= [spike_args, True]  # [spike_args, to_plot]\n",
        "\n",
        "        func_dict['IC - R input']= input_resistance_analyzer\n",
        "        arg_dict['IC - R input']= [[-30, 10] ,True]  # [dVm_limits, to_plot]\n",
        "\n",
        "        func_dict['VC - Multi IV - 150ms'] = IV_analyzer_v2\n",
        "        arg_dict['VC - Multi IV - 150ms']= [{'IV_Early':(16.5, 30),'IV_Steady_State':(100,120)} ,[False, True]]  # [measure_windows, to_plot]\n",
        "\n",
        "\n",
        "        '''Analyze Dataset'''\n",
        "        abf_recordings_df, problem_recs = analysis_iterator(abf_recordings_df,func_dict,arg_dict,verbose=verbose)\n",
        "        # clear_output(wait=True)\n",
        "        print('problem_recs')\n",
        "        _=[print('     '+r) for r in problem_recs]\n",
        "        results['problem_recs'] = problem_recs\n",
        "        results['abf_recordings_df'] = abf_recordings_df\n",
        "\n",
        "        '''Download Analysis figs'''\n",
        "        zip_name = '/content/' + data_name + '_Saved_Figs.zip'\n",
        "        !zip -r $zip_name /content/Saved_Figs\n",
        "        # clear_output()\n",
        "        files.download(data_name + '_Saved_Figs.zip')\n",
        "\n",
        "        '''Sort Cells'''\n",
        "        cell_df = cell_sorting(abf_recordings_df)\n",
        "        results['cell_df'] = cell_df\n",
        "\n",
        "        '''Consolidate to Cells'''\n",
        "        list_types = ['Recording_name','protocol','abf_timestamp', 'channelList']\n",
        "        any_types = [] + dataset['file_naming_scheme']\n",
        "        cell_df_con = cell_consolidation(cell_df,list_types,any_types)\n",
        "        results['cell_df_con'] = cell_df_con\n",
        "\n",
        "        '''Simplify IV Data'''\n",
        "        cols_to_simplify = ['IV_Early', 'IV_Steady_State']\n",
        "        cell_df_nd = simplify_dicts(cell_df_con,cols_to_simplify)\n",
        "        results['cell_df_nd'] = cell_df_nd\n",
        "\n",
        "        '''Make Excell Friendly'''\n",
        "        keys_and_data_cols={'Stim_Levels_(pA)': ['Stim_Levels_(pA)', 'Spike_Counts' ],\n",
        "                        'IV_Early_(V_stim)': ['IV_Early_(V_stim)', 'IV_Early_(I_peak)', 'IV_Steady_State_(I_mean)']}\n",
        "        cell_df_csv = csv_frinedly(cell_df_nd,keys_and_data_cols)\n",
        "        results['cell_df_csv'] = cell_df_csv\n",
        "\n",
        "\n",
        "        ''' Convert to Current Density'''\n",
        "        size_col = 'Cmq_160.0'\n",
        "        current_col_list = ['IV_Early_(I_peak)_', 'IV_Steady_State_(I_mean)_']\n",
        "        cell_df_csv = current_density_correction(cell_df_csv, size_col, current_col_list)\n",
        "        results['cell_df_csv'] = cell_df_csv\n",
        "\n",
        "        '''Abridge DataFrame'''\n",
        "        abrg_exclusions = ['Recording_name',\n",
        "                        'protocol', 'abf_timestamp', 'channelList',  'Ra_10.0', 'Rm_10.0', 'tau_10.0', 'Cmq_10.0', 'Cmf_10.0',\n",
        "                        'Cmqf_10.0',  'Cmf_160.0', 'Cmqf_160.0', 'Cm_pc_160.0',\n",
        "                        'Gain_R2', 'Stim_Levels_(pA)', 'Spike_Counts',  'Gain_Vh',  'Vhold_spike',\n",
        "                            'Rin_Rsqr',  'Ramp_AP_thresh', 'Ramp_Vh', 'Ramp_Rheobase',\n",
        "                        'v_half','is_compensated','sum_delta'\n",
        "                        'IV_Early_(range)', 'IV_Early_(I_peak)', 'IV_Early_(I_mean)', 'IV_Early_(V_stim)', 'IV_Steady_State_(range)',\n",
        "                        'IV_Steady_State_(I_peak)', 'IV_Steady_State_(I_mean)', 'IV_Steady_State_(V_stim)', ]\n",
        "\n",
        "        abrg_keep = [c for c in cell_df_csv.columns if c not in abrg_exclusions]\n",
        "        cell_df_csv_abrg = cell_df_csv[abrg_keep]\n",
        "        results['cell_df_csv_abrg'] = cell_df_csv_abrg\n",
        "\n",
        "        '''Stratify Cells By Type'''\n",
        "        strat_df_dict = stratify_rec(cell_df_csv_abrg,strat_cols)\n",
        "        strat_df_dict,_ = flatten_dict(strat_df_dict,{})\n",
        "        write_strat_dfs(strat_df_dict, dataset['data_name']+'_results_stratified')\n",
        "        results['strat_df_dict'] = strat_df_dict\n",
        "    except: return results\n",
        "    return results"
      ]
    }
  ]
}